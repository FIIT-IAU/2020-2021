{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Outlier detection\n",
    "\n",
    "- An outlier is an observation that is unlike the other observations. \n",
    "- They are rare, distinct, or do not fit in some way\n",
    "- Outliers can be measurement or input error, data corruption ot true outlier observation\n",
    "\n",
    "Identifying outliers and bad data one of the most difficult parts of data cleanup, and it takes time to get right.\n",
    "\n",
    "**This task need to be done cautiously**.\n",
    "\n",
    "\n",
    "## 1.1 Standard Deviation Method\n",
    "**3x standard deviations ($\\sigma$) from the mean ($\\mu$)** is a common cut-off in practice for identifying outliers in a Gaussian or Gaussian-like distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed \n",
    "from numpy.random import randn \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import seaborn as sns\n",
    "\n",
    "seed(1)\n",
    "data = 5 * randn(1000) + 50\n",
    "sns.distplot(data)\n",
    "\n",
    "# calculate summary statistics\n",
    "data_mean, data_std = mean(data), std(data)\n",
    "print('len=', len(data), 'mean=', data_mean, 'std=', data_std)\n",
    "\n",
    "# define outliers\n",
    "cut_off = data_std * 3\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off \n",
    "print('cutoff=', cut_off, 'lower=', lower, 'upper=', upper)\n",
    "\n",
    "# identify outliers\n",
    "outliers = [x for x in data if x < lower or x > upper] \n",
    "print('Identified outliers: %d' % len(outliers))\n",
    "\n",
    "# remove outliers\n",
    "outliers_removed = [x for x in data if x >= lower and x <= upper]\n",
    "print('Non-outlier observations: %d' % len(outliers_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Interquartile Range Method\n",
    "Limits on the sample values that are a factor $k=1.5$ of the $IQR$ below the $25$ percentile or above $75$ percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from numpy.random import randn \n",
    "from numpy import percentile\n",
    "\n",
    "seed(1)\n",
    "data = 5 * randn(1000) + 50\n",
    "sns.distplot(data)\n",
    "print('len=', len(data))\n",
    "      \n",
    "# calculate interquartile range\n",
    "q25, q75 = percentile(data, 25), percentile(data, 75)\n",
    "iqr = q75 - q25\n",
    "print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr)) \n",
    "\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 1.5\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "print('cutoff=', cut_off, 'lower=', lower, 'upper=', upper)\n",
    "\n",
    "# identify outliers\n",
    "outliers = [x for x in data if x < lower or x > upper] \n",
    "print('Identified outliers: %d' % len(outliers))\n",
    "\n",
    "# remove outliers\n",
    "outliers_removed = [x for x in data if x >= lower and x <= upper] \n",
    "print('Non-outlier observations: %d' % len(outliers_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Imputation of missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace 0 by numpy NaN in several columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "\n",
    "dataset = read_csv('data/pima-indians-diabetes.csv', header=None) \n",
    "print(dataset.head(20))\n",
    "\n",
    "num_missing = (dataset[[1,2,3,4,5,6,7]] == 0).sum()\n",
    "print(num_missing)\n",
    "\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, np.nan)\n",
    "print(dataset.isnull().sum())\n",
    "print(dataset.head(20))\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Remove rows with empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dropna(inplace=True)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Statistical Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple imputer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "data = [[7, 2, 3], [4, np.nan, 6], [10, 5, 9]]\n",
    "print(data)\n",
    "\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(data)\n",
    "\n",
    "X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n",
    "print(imp_mean.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple imputer with horse-colic dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import isnan\n",
    "from pandas import read_csv\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv('data/horse-colic.csv', header=None, na_values='?') \n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# summarize total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten())) \n",
    "\n",
    "# define imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "\n",
    "# transform the dataset\n",
    "X_trans = imputer.transform(X)\n",
    "\n",
    "# summarize total missing\n",
    "print('Missing: %d' % sum(isnan(X_trans).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**kNN imputation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import isnan\n",
    "from pandas import read_csv\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv('data/horse-colic.csv', header=None, na_values='?') \n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# summarize total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))\n",
    "\n",
    "# define imputer\n",
    "imputer = KNNImputer()\n",
    "\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "\n",
    "# transform the dataset\n",
    "Xtrans = imputer.transform(X)\n",
    "\n",
    "# summarize total missing\n",
    "print('Missing: %d' % sum(isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Transformations\n",
    "**ML models learn a mapping from input variables to an output variable**\n",
    "\n",
    "\n",
    "## 3.1 Encoding\n",
    "\n",
    "URL https://contrib.scikit-learn.org/category_encoders/\n",
    "\n",
    "**Poznamka: Python 3.8 FutureWarning is OK for the library category_encoders**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Ordinal Encoding or Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'city' : ['delhi', 'hyderabad', 'delhi', 'delhi', 'gurgaon', 'hyderabad']\n",
    "})\n",
    "\n",
    "# create an object of the OrdinalEncoding\n",
    "ce_ordinal = ce.OrdinalEncoder(cols=['city'])\n",
    "\n",
    "# fit and transform and you will get the encoded data\n",
    "ce_ordinal.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'gender' : ['M', 'F', 'M', 'F', 'F']\n",
    "})\n",
    "\n",
    "# create an object of the OneHotEncoder\n",
    "ce_OHE = ce.OneHotEncoder(cols=['gender'])\n",
    "\n",
    "# fit and transform and you will get the encoded data\n",
    "ce_OHE.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Binary Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "\n",
    "# make some data\n",
    "data = pd.DataFrame({\n",
    "    'class' : ['a', 'b', 'a', 'b', 'd', 'e', 'd', 'f', 'g', 'h', 'h', 'k', 'h', 'i', 's', 'p', 'z']})\n",
    "\n",
    "# create object of BinaryEncoder\n",
    "ce_binary = ce.BinaryEncoder(cols = ['class'])\n",
    "\n",
    "# fit and transform and you will get the encoded data\n",
    "ce_binary.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 BaseN Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "\n",
    "# make some data\n",
    "data = pd.DataFrame({\n",
    "    'class' : ['a', 'b', 'a', 'b', 'd', 'e', 'd', 'f', 'g', 'h', 'h', 'k', 'h', 'i', 's', 'p', 'z']})\n",
    "\n",
    "# create an object of the BaseNEncoder\n",
    "ce_baseN4 = ce.BaseNEncoder(cols=['class'], base=4)\n",
    "\n",
    "# fit and transform and you will get the encoded data\n",
    "ce_baseN4.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Hashing (md5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'color' : ['Yellow', 'Black', 'Green', 'Blue', 'Blue', 'Green', 'Black', 'Blue']\n",
    "})\n",
    "\n",
    "# create an object of the HashingEncoder\n",
    "ce_HE = ce.HashingEncoder(cols=['color'],n_components=5)\n",
    "\n",
    "# fit and transform and you will get the encoded data\n",
    "ce_HE.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6 Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'color' : ['Blue', 'Black', 'Black','Blue', 'Blue'],\n",
    "    'outcome' : [1,      2,        1,     1,      2,]\n",
    "})\n",
    "\n",
    "# column to perform encoding\n",
    "X = data['color']\n",
    "Y = data['outcome']\n",
    "\n",
    "# create an object of the TargetEncoder\n",
    "ce_TE = ce.TargetEncoder(cols=['color'])\n",
    "\n",
    "# fit and transform and you will get the encoded data\n",
    "ce_TE.fit_transform(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.7 Leave One Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'color' : ['Blue', 'Black', 'Black','Blue', 'Blue'],\n",
    "    'outcome' : [2,      1,        1,     1,      2]\n",
    "})\n",
    "\n",
    "# column to perform encoding\n",
    "X = data['color']\n",
    "Y = data['outcome']\n",
    "\n",
    "# create an object of the TargetEncoder\n",
    "ce_TE = ce.LeaveOneOutEncoder(cols=['color'])\n",
    "\n",
    "# fit and transform and you will get the encoded data\n",
    "ce_TE.fit_transform(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Data scaling\n",
    "\n",
    "- The scale and distribution of the data drawn from the domain may be different for each variable\n",
    "- This can lead to unstable or poor performance of models\n",
    "- This can lead to increasing sensitivity to inputs\n",
    "- This can lead to higher generalization error\n",
    "\n",
    "\n",
    "**3.2.1 Data Normalization**\n",
    "\n",
    "## $x_{normalization}=\\frac{x-x_{min}}{x_{max} - x_{min}}$\n",
    "\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "# define data\n",
    "data = asarray([[100, 0.001],\n",
    "[8, 0.05], [50, 0.005], [88, 0.07], [4, 0.1]])\n",
    "print(data)\n",
    "\n",
    "# define min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# transform data\n",
    "scaled = scaler.fit_transform(data) \n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.2 Data Standardization**\n",
    "## $x_{standardized} = \\frac{x -\\mu}{\\sigma}$\n",
    "where \n",
    "- $\\mu$ is the mean  of $x$\n",
    "- $\\sigma$ is the standard deviation of $x$\n",
    "\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# define data\n",
    "data = asarray([[100, 0.001], [8, 0.05], [50, 0.005], [88, 0.07], [4, 0.1]])\n",
    "print(data)\n",
    "\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# transform data\n",
    "scaled = scaler.fit_transform(data) \n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.3 Robust Scaling**\n",
    "\n",
    "Robust scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n",
    "\n",
    "## $x_{robust} = \\frac{x - median}{p_{75} - p_{25}}$\n",
    "\n",
    "where \n",
    "- $median$ is the median of $x$\n",
    "- $p_{25}$ and $p_{75}$ are 25th and 75th interquartile range values (IQR).\n",
    "\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# define data\n",
    "X = [[ 1., -2.,  2.],\n",
    "     [ -2.,  1.,  3.],\n",
    "     [ 4.,  1., -2.]]\n",
    "print(X)\n",
    "\n",
    "# define robust scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# transform data\n",
    "scaled = RobustScaler().fit_transform(X)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Making data distributions more Gaussian (normal)\n",
    "\n",
    "Many ML algorithms perform better or when features are on a relatively similar to normally (Gaussian) distributed.\n",
    "\n",
    "### 3.3.1 Power Transformer \n",
    "- Replacing the data with the log, square root, or inverse to remove skew\n",
    "- Yeo-Johnson transform (default): works with positive and negative values\n",
    "- Box-Cox transform: only works with strictly positive values\n",
    "- λ = −1.0 is a reciprocal transform.\n",
    "- λ = −0.5 is a reciprocal square root transform.  \n",
    "- λ = 0.0 is a log transform.\n",
    "- λ = 0.5 is a square root transform.\n",
    "- λ = 1.0 is no transform.\n",
    "\n",
    "**Random data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import exp\n",
    "from numpy.random import randn\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate gaussian data sample\n",
    "data = randn(1000)\n",
    "\n",
    "# add a skew to the data distribution\n",
    "data = exp(data)\n",
    "\n",
    "# histogram of the raw data with a skew\n",
    "pyplot.hist(data, bins=25)\n",
    "\n",
    "# reshape data to have rows and columns\n",
    "data = data.reshape((len(data),1))\n",
    "\n",
    "# power transform the raw data\n",
    "power = PowerTransformer(method='yeo-johnson', standardize=True) \n",
    "data_trans = power.fit_transform(data)\n",
    " \n",
    "# histogram of the transformed data\n",
    "pyplot.hist(data_trans, bins=25) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sonar data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import PowerTransformer \n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "# Load dataset\n",
    "dataset = read_csv('data/sonar.csv', header=None)\n",
    "\n",
    "# retrieve just the numeric input values\n",
    "data = dataset.values[:, :-1]\n",
    "\n",
    "# perform a yeo-johnson transform of the dataset \n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "data = pt.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe\n",
    "dataset = DataFrame(data)\n",
    "\n",
    "# histograms of the variables\n",
    "fig = dataset.hist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Quantile Transformer\n",
    "\n",
    "- Transform features using quantiles information.\n",
    "- This method transforms the features to follow a uniform or a normal distribution. \n",
    "- Therefore, for a given feature, this transformation tends to spread out the most frequent values. \n",
    "- It also reduces the impact of (marginal) outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(100, 1)), axis=0)\n",
    "pyplot.hist(X, bins=10)\n",
    "\n",
    "qt = QuantileTransformer(n_quantiles=10, random_state=0)\n",
    "X_t = qt.fit_transform(X)\n",
    "pyplot.hist(X_t, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/)\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
